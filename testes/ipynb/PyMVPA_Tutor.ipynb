{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "A Dataset is the basic data container in PyMVPA. It serves as the primary form of input data storage, but also as a container for more complex results returned by some algorithm. In this tutorial part we will take a look at what a dataset consists of, and how it works.\n",
      "\n",
      "Most datasets in PyMVPA are represented as a two-dimensional array, where the first axis is the samples axis, and the second axis represents the features of the samples. In the simplest case, a dataset only contains data that is a matrix of numerical values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mvpa2.tutorial_suite import *\n",
      "\n",
      "data = [[  1,  1, -1], [  2,  0,  0], [  3,  1,  1], [  4,  0, -1]]\n",
      "\n",
      "ds = Dataset(data)\n",
      "print ds.shape\n",
      "print ds.samples\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(4, 3)\n",
        "[[ 1  1 -1]\n",
        " [ 2  0  0]\n",
        " [ 3  1  1]\n",
        " [ 4  0 -1]]\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In the above example, every row vector in the data matrix becomes an observation, a sample, in the dataset, and every column vector represents an individual variable, a feature. The concepts of samples and features are essential for a dataset, hence we take a further, closer look.\n",
      "\n",
      "The dataset assumes that the first axis of the data is to be used to define individual samples. If the dataset is created using a one-dimensional vector it will therefore have as many samples as elements in the vector, and only one feature."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "one_d = [ 0, 1, 2, 3 ]\n",
      "one_ds = Dataset(one_d)\n",
      "print one_ds.shape\n",
      "print one_ds.samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(4, 1)\n",
        "[[0]\n",
        " [1]\n",
        " [2]\n",
        " [3]]\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "On the other hand, if a dataset is created from multi-dimensional data, only its second axis represents the features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "a = np.random.random((3, 4, 2, 3))\n",
      "m_ds = Dataset(a)\n",
      "print m_ds.shape\n",
      "print '\\nfeatures:\\n ',m_ds.nfeatures"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 4, 2, 3)\n",
        "\n",
        "features:\n",
        "  4\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Attributes\n",
      "What we have seen so far does not really warrant the use of a dataset over a plain array or a matrix with samples. However, in the MVPA context we often need to know more about each sample than just the value of its features. In the previous tutorial part we have already seen that per-sample target values are required for supervised-learning algorithms, and that a dataset often has to be split based on the origin of specific groups of samples. For this type of auxiliary information a dataset can also contain collections of three types of attributes: sample attribute, feature attribute, and dataset attribute.\n",
      "\n",
      "For Samples\n",
      "Each sample in a dataset can have an arbitrary number of additional attributes. They are stored as vectors of the same length as the number of samples in a collection, and are accessible via the sa attribute. A collection is derived from a standard Python dict, and hence adding sample attributes works identically to adding elements to a dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds.sa['some_attr'] = [ 0., 1, 1, 3 ]\n",
      "print ds.sa.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['some_attr']\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "However, sample attributes are not directly stored as plain data, but for various reasons as a so-called Collectable that in turn embeds a NumPy array with the actual attribute:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(ds.sa['some_attr'])\n",
      "print ds.sa['some_attr'].value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  1.  1.  3.]\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This \u201ccomplication\u201d is done to be able to extend attributes with additional functionality that is often needed and can offer a significant speed-up of processing. For example, sample attributes carry a list of their unique values. This list is only computed once (upon first request) and can subsequently be accessed directly without repeated and expensive searches:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds.sa['some_attr'].unique"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  1.  3.]\n"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "However, for most interactive uses of PyMVPA this type of access to attributes\u2019 .value is relatively cumbersome (too much typing), therefore collections offer direct access by name:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'de um jeito mais facil: ',ds.sa.some_attr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "de um jeito mais facil:  [ 0.  1.  1.  3.]\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Another purpose of the sample attribute collection is to preserve data integrity, by disallowing improper attributes:\n",
      "\n",
      "Ex.:\n",
      "\n",
      ">>> ds.sa['invalid'] = [ 1, 2, 3, 4, 5, 6 ]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.6/doctest.py\", line 1253, in __run\n",
      "    compileflags, 1) in test.globs\n",
      "  File \"<doctest tutorial_datasets.rst[21]>\", line 1, in <module>\n",
      "    ds.sa['invalid'] = [ 1, 2, 3, 4, 5, 6 ]\n",
      "  File \"/home/test/pymvpa/mvpa2/base/collections.py\", line 468, in __setitem__\n",
      "    str(self)))\n",
      "ValueError: Collectable 'invalid' with length [6] does not match the required length [4] of collection '<SampleAttributesCollection: some_attr>'.\n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "But other than basic plausibility checks, no further constraints on values of samples attributes exist. As long as the length of the attribute vector matches the number of samples in the dataset, and the attributes values can be stored in a NumPy array, any value is allowed. For example, it is perfectly possible and supported to store literal attributes. It should also be noted that each attribute may have its own individual data type, hence it is possible to have literal and numeric attributes in the same dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds.sa['literal'] = ['one', 'two', 'three', 'four']\n",
      "sorted(ds.sa.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 126,
       "text": [
        "['literal', 'some_attr']"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for attr in ds.sa: \n",
      "   print \"%s: %s\" % (attr, ds.sa[attr].value.dtype.name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "literal: string40\n",
        "some_attr: float64\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "For Features\n",
      "Feature attributes are almost identical to sample attributes, the only difference is that instead of having one attribute value per sample, feature attributes have one value per (guess what? ...) feature. Moreover, they are stored in a separate collection in the dataset that is called fa:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds.nfeatures"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3\n"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds.fa['my_fav'] = [0, 1, 0]\n",
      "ds.fa['responsible'] = ['me', 'you', 'nobody']\n",
      "print sorted(ds.fa.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['my_fav', 'responsible']\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "For The Dataset\n",
      "Finally, there can be also attributes, not per each sample, or each feature, but for the dataset as a whole: so called dataset attributes. Both assigning such attributes and accessing them later on work in exactly the same way as for the other two types of attributes, except that dataset attributes are stored in their own collection which is accessible via the a property of the dataset. However, in contrast to sample and feature attribute, no constraints on the type or size are imposed \u2013 anything can be stored. Let\u2019s store a list with all files in the current directory, just because we can:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from glob import glob\n",
      "ds.a['pointless'] = glob(\"*\")\n",
      "'setup.py' in ds.a.pointless"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 130,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Slicing, resampling, feature selection\n",
      "At this point we can already construct a dataset from simple arrays and enrich it with an arbitrary number of additional attributes. But just having a dataset isn\u2019t enough. We often need to be able to select subsets of a dataset for further processing.\n",
      "\n",
      "Slicing a dataset (i.e. selecting specific subsets) is very similar to slicing a NumPy array. It actually works almost identically. A dataset supports Python\u2019s slice syntax, but also selection by boolean masks and indices. The following three slicing operations result in equivalent output datasets, by always selecting every other samples in the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# original\n",
      "print 'Original:\\n',ds.samples\n",
      "\n",
      "# Python-style slicing\n",
      "print '\\nds[::2].samples:\\n ', ds[::2].samples\n",
      "\n",
      "# Boolean mask array\n",
      "mask = np.array([True, False, True, False])\n",
      "\n",
      "print '\\nds[mask].samples:\\n', ds[mask].samples\n",
      "\n",
      "# Slicing by index -- Python indexing start with 0 !!\n",
      "ds[[0, 2]].samples\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original:\n",
        "[[ 1  1 -1]\n",
        " [ 2  0  0]\n",
        " [ 3  1  1]\n",
        " [ 4  0 -1]]\n",
        "\n",
        "ds[::2].samples:\n",
        "  [[ 1  1 -1]\n",
        " [ 3  1  1]]\n",
        "\n",
        "ds[mask].samples:\n",
        "[[ 1  1 -1]\n",
        " [ 3  1  1]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "array([[ 1,  1, -1],\n",
        "       [ 3,  1,  1]])"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise Search the NumPy documentation for the difference between \u201cbasic slicing\u201d and \u201cadvanced indexing\u201d. The aspect of memory consumption, especially, applies to dataset slicing as well, and being aware of this fact might help to write more efficient analysis scripts. Which of the three slicing approaches above is the most memory-efficient? Which of the three slicing approaches above might lead to unexpected side-effects if the output dataset gets modified?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All three slicing-styles are equally applicable to the selection of feature subsets within a dataset. Remember, features are represented on the second axis of a dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Original:\\n',ds.samples\n",
      "print '\\nds[:, [1,2]].samples:', ds[:, [1,2]].samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original:\n",
        "[[ 1  1 -1]\n",
        " [ 2  0  0]\n",
        " [ 3  1  1]\n",
        " [ 4  0 -1]]\n",
        "\n",
        "ds[:, [1,2]].samples: [[ 1 -1]\n",
        " [ 0  0]\n",
        " [ 1  1]\n",
        " [ 0 -1]]\n"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "By applying a selection by indices to the second axis, we can easily get the last two features of our example dataset. Please note that the : is supplied for the first axis slicing. This is the Python way to indicate take everything along this axis, thus including all samples.\n",
      "\n",
      "As you can guess, it is also possible to select subsets of samples and features at the same time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subds = ds[[0,1], [0,2]]\n",
      "subds.samples\n",
      "array([[ 1, -1],\n",
      "       [ 2,  0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "array([[ 1, -1],\n",
        "       [ 2,  0]])"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "If you have prior experience with NumPy you might be confused now. What you might have expected is this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds.samples[[0,1], [0,2]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 134,
       "text": [
        "array([1, 0])"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The above code applies the same slicing directly to the NumPy array of .samples, and the result is fundamentally different. For NumPy arrays this style of slicing allows selection of specific elements by their indices on each axis of an array. For PyMVPA\u2019s datasets this mode is not very useful, instead we typically want to select rows and columns, i.e. samples and features given by their indices."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise Try to select samples [0,1] and features [0,2] simultaneously using dataset slicing. Now apply the same slicing to the samples array itself (ds.samples) \u2013 make sure that the result doesn\u2019t surprise you and find a pure NumPy way to achieve similar selection."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "One last interesting thing to look at, in the context of dataset slicing, are the attributes. What happens to them when a subset of samples and/or features is chosen? Our original dataset had both samples and feature attributes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds.sa.some_attr\n",
      "print ds.fa.responsible"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  1.  1.  3.]\n",
        "['me' 'you' 'nobody']\n"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Now let\u2019s look at what they became in the subset-dataset we previously created:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print subds.sa.some_attr\n",
      "print subds.fa.responsible"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  1.]\n",
        "['me' 'nobody']\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We see that both attributes are still there and, moreover, also the corresponding subsets have been selecte"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Load fMRI data\n",
      "Enough theoretical foreplay \u2013 let\u2019s look at a concrete example with an fMRI dataset. PyMVPA has several helper functions to load data from specialized formats, and the one for fMRI data is fmri_dataset(). The example dataset we are going to look at is a single subject from Haxby et al. (2001). For more convenience and less typing, we first specify the path of the directory with the fMRI data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tutorial_data_path = '/home/sidon/etc/tutor_pymvpa'\n",
      "path=os.path.join(tutorial_data_path, 'data')\n",
      "\n",
      "ds = fmri_dataset(os.path.join(path, 'bold.nii.gz'))\n",
      "print 'len(ds):\\n',len(ds)\n",
      "print '\\nds.nfeatures:\\n',ds.nfeatures\n",
      "print '\\nds.shape:\\n',ds.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "len(ds):\n",
        "1452\n",
        "\n",
        "ds.nfeatures:\n",
        "163840\n",
        "\n",
        "ds.shape:\n",
        "(1452, 163840)\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We can notice two things. First \u2013 it worked! Second, we obtained a two-dimensional dataset with 1452 samples (these are volumes in the NIfTI file), and over 160k features (these are voxels in the volume). The voxels are represented as a one-dimensional vector, and it seems that they have lost their association with the 3D-voxel-space. However, this is not the case, as we will see later. PyMVPA represents data in this simple format to make it compatible with a vast range of generic algorithms that expect data to be a simple matrix.\n",
      "\n",
      "We loaded all data from that NIfTI file, but usually we would be interested in a subset only, i.e. \u201cbrain voxels\u201d. fmri_dataset is capable of performing data masking. We just need to specify a mask image. Such a mask image is generated in pretty much any fMRI analysis pipeline \u2013 may it be a full-brain mask computed during skull-stripping, or an activation map from a functional localizer. We are going to use the original GLM-based localizer mask of ventral temporal cortex from Haxby et al. (2001). Let\u2019s reload the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds = fmri_dataset(os.path.join(path, 'bold.nii.gz'),mask=os.path.join(path, 'mask_vt.nii.gz'))\n",
      "print len(ds)\n",
      "print ds.nfeatures\n",
      "\n",
      "print ds.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1452\n",
        "577\n",
        "(1452, 577)\n"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected, we get the same number of samples, but now only 577 features \u2013 voxels corresponding to non-zero elements in the mask image. Now, let\u2019s explore this dataset a little further."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise Explore the dataset attribute collections. What kind of information do they contain?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds.sa.keys()\n",
      "print ds.sa.time_coords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['time_indices', 'time_coords']\n",
        "[  0.00000000e+00   2.50000000e+00   5.00000000e+00 ...,   3.62250000e+03\n",
        "   3.62500000e+03   3.62750000e+03]\n"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Besides samples, the dataset offers a number of attributes that enhance the data with information that is present in the NIfTI image file header. Each sample has information about its volume index in the time series and the actual acquisition time (relative to the beginning of the file). Moreover, the original voxel index (sometimes referred to as ijk) for each feature is available too. Finally, the dataset also contains information about the dimensionality of the input volumes, voxel size, and any other NIfTI-specific information since it also includes a dump of the full NIfTI image header."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds.sa.time_indices[:5]\n",
      "print ds.sa.time_coords[:5]\n",
      "print ds.fa.voxel_indices[:5]\n",
      "print ds.a.voxel_eldim\n",
      "print ds.a.voxel_dim\n",
      "'imghdr' in ds.a\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 2 3 4]\n",
        "[  0.    2.5   5.    7.5  10. ]\n",
        "[[ 6 23 24]\n",
        " [ 7 18 25]\n",
        " [ 7 18 26]\n",
        " [ 7 18 27]\n",
        " [ 7 19 25]]\n",
        "(3.5, 3.75, 3.75)\n",
        "(40, 64, 64)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 155,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "in addition to all this information, the dataset also carries a key additional attribute: the mapper. A mapper is an important concept in PyMVPA, and hence has its own tutorial chapter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds.a.mapper"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Chain: <Flatten>-<StaticFeatureSelection>>\n"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Having all these attributes being part of a dataset is often a useful thing to have, but in some cases (e.g. when it comes to efficiency, and/or very large datasets) one might want to have a leaner dataset with just the information that is really necessary. One way to achieve this, is to strip all unwanted attributes. The Dataset class\u2019 copy() method can help with that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stripped = ds.copy(deep=False, sa=['time_coords'], fa=[], a=[])\n",
      "print stripped\n",
      "print ds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Dataset: 1452x577@int16, <sa: time_coords>>\n",
        "<Dataset: 1452x577@int16, <sa: time_coords,time_indices>, <fa: voxel_indices>, <a: imghdr,imgtype,mapper,voxel_dim,voxel_eldim>>\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We can see that all attributes besides time_coords have been filtered out. Setting the deep arguments to False causes the copy function to reuse the data from the source dataset to generate the new stripped one, without duplicating all data in memory \u2013 meaning both datasets now share the sample data and any change done to ds will also affect stripped."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "intermediate Storage\n",
      "Some data preprocessing can take a long time. One would rather prevent having to do it over and over again, and instead just store the preprocessed data into a file for subsequent analyses. PyMVPA offers functionality to store a large variety of objects, including datasets, into HDF5 files. A variant of this format is also used by recent versions of Matlab to store data.\n",
      "\n",
      "For HDF5 support, PyMVPA depends on the h5py package. If it is available, any dataset can be saved to a file by simply calling save() with the desired filename."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tempfile, shutil\n",
      "# create a temporary directory\n",
      "tempdir = tempfile.mkdtemp()\n",
      "print tempdir\n",
      "ds.save(os.path.join(tempdir, 'mydataset.hdf5'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/tmp/tmprlAoN8\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "HDF5 is a flexible format that also supports, for example, data compression. To enable it, you can pass additional arguments to save() that are supported by h5py\u2019s Group.create_dataset(). Instead of using save() one can also use the h5save() function in a similar way. Saving the same dataset with maximum gzip-compression looks like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds.save(os.path.join(tempdir, 'mydataset.gzipped.hdf5'), compression=9)\n",
      "h5save(os.path.join(tempdir, 'mydataset.gzipped.hdf5'), ds, compression=9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Loading datasets from a file is easy too. h5load() takes a filename as an argument and returns the stored dataset. Compressed data will be handled transparently."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loaded = h5load(os.path.join(tempdir, 'mydataset.hdf5'))\n",
      "np.all(ds.samples == loaded.samples)\n",
      "\n",
      "# cleanup the temporary directory, and everything it includes\n",
      "shutil.rmtree(tempdir, ignore_errors=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 161
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Note that this type of dataset storage is not appropriate from long-term archival of data, as it relies on a stable software environment. For long-term storage, use other formats."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}